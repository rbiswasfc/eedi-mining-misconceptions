seed: 48778
debug: false
save_model: true
use_wandb: false
local_rank: 0
enable_cuda_optimizations: true

fold: 0
full_fit: false

use_distillation: false
use_cot: true
temperature: 0.95

dataset:
  fold_dataset: conjuring92/eedi-five-folds
  input_dataset: eedi-mining-misconceptions-in-mathematics
  comp_dataset: eedi-mining-misconceptions-in-mathematics
  label_dataset: eedi-mining-misconceptions-in-mathematics

model:
  backbone_path: Qwen/Qwen2.5-72B
  max_length: 672
  num_labels: 1
  num_proc: 8
  use_gradient_checkpointing: true
  compile_model: true # true # false
  trust_remote_code: false
  attn_implementation: flash_attention_2

  tokenizer:
    truncation_side: left
    use_fast: true

  use_bnb: true
  use_lora: true
  k_shot: 1
  
  lora:
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - up_proj
      - down_proj
      - gate_proj
    r: 64
    lora_alpha: 128
    lora_dropout: 0.01
    use_dora: false
    
    modules_to_save: []
      # - lm_head
      # - embed_tokens
      
train_params:
  per_device_train_group_size: 8 # 16
  per_device_train_batch_size: 1
  sub_batch_size: 4
  per_device_eval_batch_size: 8
  num_train_epochs: 1
  gradient_accumulation_steps: 2 # 1

  warmup_pct: 0.02
  eval_frequency: 1000

optimizer:
  name: AdamW8bit

  lr: 1e-6
  lr_lora_a: 1e-5
  lr_lora_b: 5e-5
  lr_embed_tokens: 1e-6

  weight_decay: 1e-2
  max_grad_norm: 64.0

  adam_beta_1: 0.9
  adam_beta_2: 0.95
  adam_epsilon: 1e-8 

outputs:
  model_dir: ../models/qwen_pointwise_72b

wandb:
  project: eedi-dev
  run_name: qwen-pointwise-72b
  tags:
    - qwen